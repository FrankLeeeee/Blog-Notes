{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-modal RAG in 60 Lines of Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a multi-modal vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoProcessor, AutoTokenizer, CLIPTextModelWithProjection, CLIPVisionModelWithProjection\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "class MultiModalVectorStore:\n",
    "\n",
    "    def __init__(self):\n",
    "        # image store\n",
    "        self.img_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.vision_model = CLIPVisionModelWithProjection.from_pretrained(\n",
    "            \"openai/clip-vit-base-patch32\", torch_dtype=torch.float16\n",
    "        ).cuda()\n",
    "        self.img_store = []\n",
    "\n",
    "        # text store\n",
    "        self.splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=1000, chunk_overlap=200)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_model = CLIPTextModelWithProjection.from_pretrained(\n",
    "            \"openai/clip-vit-base-patch32\", torch_dtype=torch.float16\n",
    "        ).cuda()\n",
    "        self.text_store = []\n",
    "        \n",
    "    def add_image(self, image: Image.Image) -> List[float]:\n",
    "        inputs = self.img_processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.vision_model(**inputs)\n",
    "        emb = outputs.image_embeds\n",
    "        self.img_store.append((image, emb))\n",
    "    \n",
    "    def add_text(self, text: str) -> List[float]:\n",
    "        chunks = self.splitter.split_text(text)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            inputs = self.tokenizer(chunk, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "            outputs = self.text_model(**inputs)\n",
    "            emb = outputs[0]\n",
    "            self.text_store.append((chunk, emb))\n",
    "\n",
    "    def retrieve(self, query: str, top_k_text_chunks: int) -> List[str]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a vision language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLM:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load the model in half-precision\n",
    "        self.model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2', torch_dtype=torch.bfloat16).cuda()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2')\n",
    "\n",
    "    def chat(self, query: str, txt_context: List[str], img: Image.Image):\n",
    "        context_prompt = \"\\n\".join([f\" - {chunk}\" for chunk in txt_context])\n",
    "        instruction_prompt = f\"\"\"You are a helpful chatbot.\n",
    "        Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "        {context_prompt}\n",
    "\n",
    "        Question: {query}\n",
    "        \"\"\"\n",
    "        msgs = [{'role': 'user', 'content': instruction_prompt}]\n",
    "        res, _, _ = self.model.chat(img, msgs, context=None, tokenizer=self.tokenizer, sampling=True, temperature=0.7\n",
    "        )\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMRag:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.store = MultiModalVectorStore()\n",
    "        self.chat_model = VLM()\n",
    "\n",
    "    def add(self, data: Union[str, Image.Image]) -> None:\n",
    "        if isinstance(data, str):\n",
    "            self.store.add_text(data)\n",
    "        else:\n",
    "            self.store.add_image(data)\n",
    "    \n",
    "    def query(self, query: str, top_n_text: int = 3, top_n_image: int = 3):\n",
    "        text_chunks, img = self.store.retrieve(query, top_n_text)\n",
    "        response = self.chat_model.chat(query, text_chunks, img)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "We can finally test our multi-modal RAG system. Let's use a wikipedia article as our data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
